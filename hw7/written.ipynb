{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-14T23:08:56.692479Z",
     "start_time": "2024-11-14T23:08:56.675541Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import six\n",
    "import torch\n",
    "import matplotlib.pyplot as plt  \n",
    "from rnn import RNNLanguageModel, train, validate, SentenceDataset\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T22:51:19.994573Z",
     "start_time": "2024-11-14T22:51:19.986975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "88b0918254df1f6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T22:51:40.149732Z",
     "start_time": "2024-11-14T22:51:40.143306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths\n",
    "train_data_path = \"data/tiny_train_stories.json\"\n",
    "val_data_path = \"data/tiny_valid_stories.json\"\n",
    "tokenizer_path = \"my_tokenizer\""
   ],
   "id": "6804d0db52b719cf",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T22:51:52.747941Z",
     "start_time": "2024-11-14T22:51:52.703756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "vocab_size = tokenizer.vocab_size"
   ],
   "id": "76037322e88b76eb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:04:05.732776Z",
     "start_time": "2024-11-14T23:04:05.703036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters to test\n",
    "configs = [\n",
    "    {\"embed_dim\": 64, \"hidden_dim\": 64},\n",
    "    {\"embed_dim\": 128, \"hidden_dim\": 128},\n",
    "    {\"embed_dim\": 256, \"hidden_dim\": 256},\n",
    "    {\"embed_dim\": 512, \"hidden_dim\": 512},\n",
    "]\n",
    "\n",
    "# Data\n",
    "train_data = SentenceDataset(train_data_path)\n",
    "val_data = SentenceDataset(val_data_path)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=1, shuffle=False)"
   ],
   "id": "99f7d395804ca1f5",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:11:36.164376Z",
     "start_time": "2024-11-14T23:11:36.119284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Store results\n",
    "train_losses = {}\n",
    "val_losses = {}"
   ],
   "id": "bd5b551f116aadd6",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '_six'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/_y/_wgnnh997rgcxb8p1b8pdf700000gn/T/ipykernel_46505/1583390781.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Loss function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mloss_fn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCrossEntropyLoss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Store results\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mtrain_losses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/10601 ML/.venv/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/10601 ML/.venv/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, weight, size_average, reduce, reduction)\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/10601 ML/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36mregister_buffer\u001B[0;34m(self, name, tensor, persistent)\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'torch' has no attribute '_six'"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:11:46.617626Z",
     "start_time": "2024-11-14T23:11:46.551432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train models for each configuration\n",
    "for config in configs:\n",
    "    print(f\"Training with embed_dim={config['embed_dim']} and hidden_dim={config['hidden_dim']}...\")\n",
    "    lm = RNNLanguageModel(\n",
    "        embed_dim=config[\"embed_dim\"],\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        vocab_size=vocab_size,\n",
    "        key_dim=32,\n",
    "        value_dim=32,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(lm.parameters(), lr=1e-3)\n",
    "    train_loss, val_loss = train(\n",
    "        lm,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        num_sequences=128,\n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "    train_losses[f\"{config['embed_dim']}_{config['hidden_dim']}\"] = train_loss\n",
    "    val_losses[f\"{config['embed_dim']}_{config['hidden_dim']}\"] = val_loss\n",
    "\n",
    "# Plotting\n",
    "sequences = list(range(1, len(train_losses[next(iter(train_losses))]) + 1))\n",
    "\n",
    "# Training loss plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for key, loss in train_losses.items():\n",
    "    plt.plot(sequences, loss, label=f\"Embed/Hidden Dim = {key.split('_')[0]}\")\n",
    "plt.xlabel(\"Number of Sequences\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss vs. Number of Sequences\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Validation loss plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for key, loss in val_losses.items():\n",
    "    plt.plot(sequences, loss, label=f\"Embed/Hidden Dim = {key.split('_')[0]}\")\n",
    "plt.xlabel(\"Number of Sequences\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Validation Loss vs. Number of Sequences\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "755a0de2524c244f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with embed_dim=64 and hidden_dim=64...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '_six'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/_y/_wgnnh997rgcxb8p1b8pdf700000gn/T/ipykernel_46505/573764431.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m         \u001B[0mvocab_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvocab_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m         \u001B[0mkey_dim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m         \u001B[0mvalue_dim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m     ).to(device)\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/10601 ML/hw7/hw7-4/handout/rnn.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, embed_dim, hidden_dim, vocab_size, key_dim, value_dim)\u001B[0m\n\u001B[1;32m    274\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    275\u001B[0m         \u001B[0;31m# TODO: Initialize word embeddings (HINT: use nn.Embedding)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 276\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membeddings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEmbedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvocab_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0membed_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    277\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    278\u001B[0m         \u001B[0;31m# RNN backbone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/10601 ML/.venv/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, device, dtype)\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/10601 ML/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__setattr__\u001B[0;34m(self, name, value)\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/10601 ML/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36mregister_parameter\u001B[0;34m(self, name, param)\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'torch' has no attribute '_six'"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a1a689a7e23e10bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
